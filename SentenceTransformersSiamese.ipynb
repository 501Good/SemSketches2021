{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Using Sentence Transformers[1] as a base architecture. The authors propose to use the average of all tokens' hidden states to produce the final sentence representation\n",
    "\n",
    "[1] Reimers, I. Gurevych (2019) Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint arXiv:1908.10084;\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.models import Pooling, Transformer\n",
    "from sentence_transformers import SentenceTransformer, models, util, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import sentence_transformers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Union, Optional\n",
    "\n",
    "SEED = 1\n",
    "torch.random.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Sentence RuBERT[2] as a starting point for the finetuning. Sentence RuBERT was trained in a similar manner to the Sentence Transformers library, i.e. using mean pooling to represent the sentence encoding. It was also finetuned on the NLI[3][4] datasets that are used for sentence similarity models.\n",
    "\n",
    "[2] http://docs.deeppavlov.ai/en/master/features/models/bert.html\n",
    "\n",
    "[3] Williams A., Bowman S. (2018) XNLI: Evaluating Cross-lingual Sentence Representations. arXiv preprint arXiv:1809.05053\n",
    "\n",
    "[4] Bowman, G. Angeli, C. Potts, and C. D. Manning. (2015) A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence_ru_cased_pipe_L-12_H-768_A-12_pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To construct the input, three additional tokens were added: `[E]`, `[/E]`, and `[BLANK]`. \n",
    "\n",
    "`[E]` and `[/E]` are used to mark the start and the end of the instance word in the context sentence. A sketch is unfolded in one flat sentence; each role and predicate token marked with `[E]` and `[/E]` from the begging and end. Additionally, to force the model to learn a more general representation of the context and sketch, $15\\%$ of the entities are randomly masked with a special `[BLANK]` symbol during training[5].\n",
    "\n",
    "The trial dataset was split into train-dev-test splits with the proportions 80-10-10. For each context in the dataset, two training pairs were constructed: the context$-$matching sketch pair with the label `1` and context$-$random sketch pair with the label `0`.\n",
    "\n",
    "### Input example:\n",
    "\n",
    "Context: `Согласитесь, что в схватке социально-близкого с социально-чуждым не может государство [E][BLANK][/E] за последнего`\n",
    "\n",
    "Sketch: `[E]Object[/E] [E]волосы[/E] [E]шерсть[/E] [E]работы[/E] [E]лед[/E] [E]гор[/E] [E]лес[/E][E]Quantity[/E] [E]меньше[/E] [E]все больше[/E] [E]чуть-чуть[/E] [E]немножко[/E] [E]еще немного[/E] [E]столько[/E][E][BLANK][/E] [E]в августе[/E] [E]в декабре[/E] [E]два[/E] [E]в коротком будущем[/E] [E]с 1907?г.[/E] [E][BLANK][/E][E]Locative_FinalPoint[/E] [E]в один ряд[/E] [E]в тупик[/E] [E]в строй[/E] [E]в свою очередь[/E] [E]на колени[/E] [E]на ноги[/E][E]Locative_Distance[/E] [E]ближе[/E] [E]поодаль[/E] [E]рядышком[/E] [E]рядом[/E] [E][BLANK][/E] [E][BLANK][/E][E][BLANK][/E] [E]дурно[/E] [E][BLANK][/E] [E]круче[/E] [E][BLANK][/E] [E]лучше[/E] [E]прекрасно[/E]`\n",
    "\n",
    "Label: `1`\n",
    "\n",
    "[5] Soares, Livio Baldini, et al. \"Matching the blanks: Distributional similarity for relation learning.\" arXiv preprint arXiv:1906.03158 (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('SemSketches/data')\n",
    "train_dir = 'trial'\n",
    "dev_dir = 'dev'\n",
    "\n",
    "contexts_name = 'contexts_{}.data'\n",
    "sketches_name = 'sketches_{}.data'\n",
    "gold_name = 'trial.gold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gold_name = 'train_trial.gold'\n",
    "dev_gold_name = 'dev_trial.gold'\n",
    "test_gold_name = 'test_trial.gold'\n",
    "\n",
    "all_labels = json.load(open(data_dir / train_dir / gold_name, encoding='utf-8'))\n",
    "gold_reverse = defaultdict(list)\n",
    "for k, v in all_labels.items():\n",
    "    gold_reverse[v].append(k)\n",
    "\n",
    "train_labels = {k: v[:80] for k, v in gold_reverse.items()}\n",
    "dev_labels = {k: v[80:90] for k, v in gold_reverse.items()}\n",
    "test_labels = {k: v[90:] for k, v in gold_reverse.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemSketchesDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 contexts_path: Path, \n",
    "                 sketches_path: Path,\n",
    "                 dev_contexts_path: Optional[Path] = None,\n",
    "                 dev_sketches_path: Optional[Path] = None,\n",
    "                 labels: Optional[Union[Path, dict]] = None, \n",
    "                 eval: Optional[bool] = False,\n",
    "                 mark_instance: Optional[bool] = False, \n",
    "                 blank_prob: Optional[float] = 0.7):\n",
    "        self.context_data = json.load(open(contexts_path, encoding='utf-8'))\n",
    "        self.sketches_data = json.load(open(sketches_path, encoding='utf-8'))\n",
    "        \n",
    "        if dev_contexts_path and dev_sketches_path:\n",
    "            self.dev_context_data = json.load(open(dev_contexts_path, encoding='utf-8'))\n",
    "            self.dev_sketches_data = json.load(open(dev_sketches_path, encoding='utf-8'))\n",
    "        \n",
    "        self.eval = eval\n",
    "        if isinstance(labels, Path):\n",
    "            self.labels_data = json.load(open(labels, encoding='utf-8'))\n",
    "        elif isinstance(labels, dict):\n",
    "            self.labels_data = labels\n",
    "        elif labels in None and self.eval:\n",
    "            self.labels_data = None\n",
    "        else:\n",
    "            raise NameError(\"Please provide either labels Path object or dict!\")\n",
    "            \n",
    "        self.blank_prob = blank_prob\n",
    "        self.mark_instance= mark_instance\n",
    "\n",
    "        context_sentences = {}\n",
    "        for context_id, context in self.context_data.items():\n",
    "            sentence = context['sentence']\n",
    "            if mark_instance:\n",
    "                start = context['start']\n",
    "                end = context['end']\n",
    "                target = '[BLANK]' if np.random.uniform() < blank_prob else sentence[start:end]\n",
    "                sentence = sentence[:start] + '[E]' + target + '[/E]' + sentence[end:]\n",
    "            context_sentences[context_id] = sentence\n",
    "        \n",
    "        if dev_contexts_path:\n",
    "            dev_context_sentences = {}        \n",
    "            for context_id, context in self.dev_context_data.items():\n",
    "                sentence = context['sentence']\n",
    "                if mark_instance:\n",
    "                    start = context['start']\n",
    "                    end = context['end']\n",
    "                    target = '[BLANK]' if np.random.uniform() < blank_prob else sentence[start:end]\n",
    "                    sentence = sentence[:start] + '[E]' + target + '[/E]' + sentence[end:]\n",
    "                dev_context_sentences[context_id] = sentence\n",
    "\n",
    "        sketch_sentences = {}\n",
    "        for sketch_id, sketch in self.sketches_data.items():\n",
    "            sketch_sentences[sketch_id] = self.construct_sketch(sketch_id)\n",
    "            \n",
    "        if dev_sketches_path:\n",
    "            dev_sketch_sentences = {}\n",
    "            for sketch_id, sketch in self.dev_sketches_data.items():\n",
    "                dev_sketch_sentences[sketch_id] = self.construct_sketch(sketch_id, dev=True)\n",
    "\n",
    "        self.data = []\n",
    "        for sketch, sents in self.labels_data.items():\n",
    "            for sent in sents:\n",
    "                self.data.append(InputExample(texts=[context_sentences[sent], sketch_sentences[sketch]], label=1.))\n",
    "\n",
    "                if dev_contexts_path and dev_sketches_path:\n",
    "                    dev_sketch = dev_sketch_sentences[random.choice(list(dev_sketch_sentences.keys()))]\n",
    "                    rand_context = context_sentences[random.choice(list(context_sentences.keys()))]\n",
    "                    self.data.append(InputExample(texts=[rand_context, dev_sketch], label=0.))\n",
    "                else:\n",
    "                    neg_sketches = list(self.labels_data.keys())\n",
    "                    neg_sketches.remove(sketch)\n",
    "                    neg_sketch = random.sample(neg_sketches, k=1)\n",
    "                    for neg in neg_sketch:   \n",
    "                        neg_sent = random.choice(self.labels_data[neg])\n",
    "                        self.data.append(InputExample(texts=[context_sentences[neg_sent], self.construct_sketch(sketch)], label=0.))\n",
    "                \n",
    "                        \n",
    "    def construct_sketch(self, sketch_id, dev=False):\n",
    "        sketch = ''\n",
    "        if dev:\n",
    "            sketches_data = self.dev_sketches_data[sketch_id]\n",
    "        else:\n",
    "            sketches_data = self.sketches_data[sketch_id]\n",
    "        for role, words in sketches_data.items():\n",
    "            if self.mark_instance:\n",
    "                role_token = '[BLANK]' if np.random.uniform() < self.blank_prob else role\n",
    "                role = '[E]' + role_token + '[/E]'\n",
    "                predicates = ['[E][BLANK][/E]' if np.random.uniform() < self.blank_prob else f\"[E]{word}[/E]\" for word in words[1]]\n",
    "                sketch += role + ' ' + ' '.join(predicates)\n",
    "            else:\n",
    "                sketch += ' '.join([role] + [word for word in words[1]])\n",
    "        return sketch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts_path = data_dir / train_dir / contexts_name.format(train_dir)\n",
    "train_sketches_path = data_dir / train_dir / sketches_name.format(train_dir)\n",
    "dev_sketches_path = data_dir / dev_dir / sketches_name.format(dev_dir)\n",
    "dev_contexts_path = data_dir / dev_dir / contexts_name.format(dev_dir)\n",
    "train_labels_path = data_dir / train_dir / gold_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_instance = True\n",
    "train_examples = SemSketchesDataset(train_contexts_path, train_sketches_path, labels=train_labels, \n",
    "                                    mark_instance=mark_instance, blank_prob=0.7)\n",
    "dev_examples = SemSketchesDataset(train_contexts_path, train_sketches_path, labels=dev_labels, \n",
    "                                  mark_instance=mark_instance, blank_prob=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Согласитесь, что в схватке социально-близкого с социально-чуждым не может государство [E][BLANK][/E] за последнего',\n",
       " '[E]Object[/E] [E]волосы[/E] [E]шерсть[/E] [E]работы[/E] [E]лед[/E] [E]гор[/E] [E]лес[/E][E]Quantity[/E] [E]меньше[/E] [E]все больше[/E] [E]чуть-чуть[/E] [E]немножко[/E] [E]еще немного[/E] [E]столько[/E][E][BLANK][/E] [E]в августе[/E] [E]в декабре[/E] [E]два[/E] [E]в коротком будущем[/E] [E]с 1907?г.[/E] [E][BLANK][/E][E]Locative_FinalPoint[/E] [E]в один ряд[/E] [E]в тупик[/E] [E]в строй[/E] [E]в свою очередь[/E] [E]на колени[/E] [E]на ноги[/E][E]Locative_Distance[/E] [E]ближе[/E] [E]поодаль[/E] [E]рядышком[/E] [E]рядом[/E] [E][BLANK][/E] [E][BLANK][/E][E][BLANK][/E] [E]дурно[/E] [E][BLANK][/E] [E]круче[/E] [E][BLANK][/E] [E]лучше[/E] [E]прекрасно[/E]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[2].texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's Architecture\n",
    "\n",
    "The model follows a siamese network architecture with a cosine similarity loss as in Sentence Transformers (Reimers et al, 2019; https://www.sbert.net/docs/training/overview.html#network-architecture). Here, instead of the mean pooling of all the tokens' representations, a custom mean pooling of the entity tokens (`[E]`) is used.\n",
    "\n",
    "The model is trained for maximum of 10 epochs with the evaluation on the dev set every 100 steps. The model with the best cosine similarity score is used in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "train_loss = losses.CosineSimilarityLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "warmup_steps = 40\n",
    "model_save_path = 'sentence_ru_cased_fine_tuned_blanks_entity_pooling_recreation_L-12_H-768_A-12_pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=100,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "For the inference, refer to the semantic search proposed by Sentence Transformers (https://www.sbert.net/examples/applications/semantic-search/README.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'sentence_ru_cased_fine_tuned_blanks_entity_pooling_L-12_H-768_A-12_pt'\n",
    "model = SentenceTransformer(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(contexts_path, mark_instance=False):\n",
    "    context_data = json.load(open(contexts_path, encoding='utf-8'))\n",
    "    context_sentences = {}\n",
    "    for context_id, context in context_data.items():\n",
    "        sentence = context['sentence']\n",
    "        if mark_instance:\n",
    "            start = context['start']\n",
    "            end = context['end']\n",
    "            target = sentence[start:end]\n",
    "            sentence = sentence[:start] + '[E]' + target + '[/E]' + sentence[end:]\n",
    "        context_sentences[context_id] = sentence\n",
    "    return context_sentences\n",
    "\n",
    "def get_sketches(sketches_path, mark_instance=False):\n",
    "    sketches_data = json.load(open(sketches_path, encoding='utf-8'))\n",
    "    sketch_sentences = defaultdict(str)\n",
    "    for sketch_id, sketch in sketches_data.items():\n",
    "        for role, words in sketch.items():\n",
    "            if mark_instance:\n",
    "                role = '[E]' + role + '[/E]'\n",
    "                predicates = [f\"[E]{word}[/E]\" for word in words[1]]\n",
    "                sketch_sentences[sketch_id] += role + ' ' + ' '.join(predicates)\n",
    "            else:\n",
    "                sketch_sentences[sketch_id] += ' '.join([role] + [word for word in words[1]])\n",
    "    return sketch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_contexts = get_contexts('SemSketches/data/dev/contexts_dev.data', True)\n",
    "dev_sketches = get_sketches('SemSketches/data/dev/sketches_dev.data', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "corpus = list(dev_sketches.values())\n",
    "corpus_ids = list(dev_sketches.keys())\n",
    "\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "corpus_embeddings = corpus_embeddings.to('cuda')\n",
    "corpus_embeddings = util.normalize_embeddings(corpus_embeddings)\n",
    "\n",
    "queries = list(dev_contexts.values())\n",
    "queries_ids = list(dev_contexts.keys())\n",
    "\n",
    "query_embeddings = model.encode(queries, convert_to_tensor=True)\n",
    "query_embeddings = query_embeddings.to('cuda')\n",
    "query_embeddings = util.normalize_embeddings(query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.dot_score, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i, hit in enumerate(hits):\n",
    "    results[queries_ids[i]] = corpus_ids[hit[0]['corpus_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results, open('task1.pred', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}